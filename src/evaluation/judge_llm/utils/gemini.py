import os
import time
import json
import google.generativeai as genai
import ollama
from typing import Dict, List

MODEL_STATE = {
    'exhausted_models': set(),
    'model_cooldowns': {},
    'last_reset_time': time.time()
}

COOLDOWN_DURATION = 3600

LOCAL_JUDGE_FALLBACK = 'gemma2:2b'

GEMINI_FALLBACK_ORDER = [
    'gemini-2.5-pro', 'gemini-2.5-flash', 'gemini-2.0-flash-lite', 'gemini-1.5-flash'
]

def setup_gemini():
    api_key = os.getenv('GEMINI_API_KEY')
    if api_key:
        genai.configure(api_key=api_key)
        return True
    return False

def is_model_available(model_name: str) -> bool:
    current_time = time.time()
    if model_name in MODEL_STATE['exhausted_models']:
        if current_time - MODEL_STATE['model_cooldowns'].get(model_name, 0) > COOLDOWN_DURATION:
            MODEL_STATE['exhausted_models'].discard(model_name)
            return True
        return False
    return True

def judge_recipe(dish_name: str, recipe_content: str, chef_name: str) -> Dict:
    """
    The core function that sends the recipe to an LLM (Gemini or Ollama) 
    to get a score (1-10) and reasoning.
    """
    if not recipe_content:
        return {"score": 0, "reasoning": "Recipe generation failed (empty output)."}

    # The Prompt for the Judge
    prompt = f"""
    Act as a strict culinary critic (just remember that the model tested is qwen 05b so it might not be as good as another model). 
    Evaluate this recipe for "{dish_name}" generated by model "{chef_name}".
    
    RECIPE:
    {recipe_content[:4000]}... (truncated if too long)

    CRITERIA:
    1. Does it actually make "{dish_name}"? (Crucial)
    2. Are the instructions logical and safe?
    3. Are the ingredients standard for this dish?
    
    Respond ONLY with this JSON format:
    {{
        "score": <int 1-10>,
        "reasoning": "<short explanation string>"
    }}
    """

    # 1. Try Gemini Models in order
    for model in GEMINI_FALLBACK_ORDER:
        if is_model_available(model):
            try:
                m = genai.GenerativeModel(model)
                # Request JSON response type for easier parsing
                resp = m.generate_content(
                    prompt, 
                    generation_config={"response_mime_type": "application/json"}
                )
                return json.loads(resp.text)
            except Exception as e:
                print(f"‚ö†Ô∏è {model} failed or quota exceeded. Switching...")
                MODEL_STATE["exhausted_models"].add(model)
                MODEL_STATE["model_cooldowns"][model] = time.time()

    # 2. Fallback to Local Ollama (Gemma) if all APIs fail
    print(f"üîå API exhausted. Falling back to local judge: {LOCAL_JUDGE_FALLBACK}")
    try:
        resp = ollama.chat(
            model=LOCAL_JUDGE_FALLBACK, 
            messages=[{"role": "user", "content": prompt}]
        )
        # Local models might output markdown like ```json ... ```
        clean_json = (
            resp["message"]["content"]
            .replace("```json", "")
            .replace("```", "")
            .strip()
        )
        return json.loads(clean_json)
    except Exception as e:
        return {"score": 0, "reasoning": f"Judge Error: {str(e)}"}
